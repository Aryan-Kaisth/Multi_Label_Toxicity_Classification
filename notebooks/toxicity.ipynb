{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c49480fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import contractions\n",
    "import emoji\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "import spacy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e8490de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data from source \n",
    "df = pd.read_csv(r\"C:\\Playground\\Toxicity_Classification\\data\\training\\train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "520ba6fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "205e62b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping irrelevant features\n",
    "df.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95051628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 duplicate instaces\n"
     ]
    }
   ],
   "source": [
    "# Checking for duplicate instances\n",
    "print(f\"There are {df.duplicated().sum()} duplicate instaces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "476e9c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for null values\n",
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1644ad9",
   "metadata": {},
   "source": [
    "### Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91a068ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a deep copy of our data\n",
    "df_copy = df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3c3441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(text):\n",
    "    \"\"\"\n",
    "    Convert the input text to lowercase.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The text to convert.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The text in lowercase.\n",
    "    \"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "df_copy[\"preprocessed_text\"] = df_copy[\"comment_text\"].apply(to_lowercase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baee3b4",
   "metadata": {},
   "source": [
    "### Removing links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e84ffec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    \"\"\"\n",
    "    Remove URLs from the input text using a regular expression.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The input text from which URLs will be removed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The text with all URLs removed.\n",
    "    \"\"\"\n",
    "    return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "\n",
    "# Apply the function to each text entry\n",
    "df_copy['preprocessed_text'] = df_copy['preprocessed_text'].apply(remove_urls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77247617",
   "metadata": {},
   "source": [
    "### Expanding Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "728fd67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    \"\"\"\n",
    "    Expand contractions in the input text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The text containing contractions to expand.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The text with all contractions expanded.\n",
    "    \"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "\n",
    "df_copy[\"preprocessed_text\"] = df_copy[\"preprocessed_text\"].apply(expand_contractions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7b6e48",
   "metadata": {},
   "source": [
    "### Removing Accents/Diacritics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72f75036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accents_diacritics(text):\n",
    "    \"\"\"\n",
    "    Remove accents and diacritics from the input text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The text to normalize.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The text with all accents and diacritics removed.\n",
    "    \"\"\"\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = ''.join([c for c in text if not unicodedata.combining(c)])\n",
    "    return text\n",
    "\n",
    "\n",
    "df_copy['preprocessed_text'] = df_copy['preprocessed_text'].apply(remove_accents_diacritics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd4c540",
   "metadata": {},
   "source": [
    "### De-emojify the emojies into respective text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e6f9e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emojis(text):\n",
    "    \"\"\"\n",
    "    Convert emojis in the input text into descriptive text labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The text containing emojis to convert.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The text with emojis replaced by their descriptive names.\n",
    "    \"\"\"\n",
    "    return emoji.demojize(text)\n",
    "\n",
    "\n",
    "df_copy['preprocessed_text'] = df_copy['preprocessed_text'].apply(convert_emojis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb6705b",
   "metadata": {},
   "source": [
    "### Removing mentions to other users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f161c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mentions(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove @mentions from the input text and normalize spacing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Input text that may contain @mentions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Text with all @mentions removed and extra spaces cleaned.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'@[A-Za-z0-9_.-]+', '', text)\n",
    "    return \" \".join(text.split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94864f0d",
   "metadata": {},
   "source": [
    "### Removing numbers and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10574947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning text (punct+numbers): 100%|\u001b[32m███████████████████████\u001b[0m| 159571/159571 [08:07<00:00, 327.39it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def spacy_remove_punct_numbers_pipe(text_list):\n",
    "    \"\"\"\n",
    "    Clean a list of text documents using spaCy by removing punctuation and numbers.\n",
    "\n",
    "    This function processes texts efficiently using spaCy's `nlp.pipe()` which supports\n",
    "    batching and multiprocessing. It removes:\n",
    "        - punctuation tokens\n",
    "        - numeric tokens\n",
    "        - any tokens that are not purely alphabetic\n",
    "\n",
    "    Each token that passes the filter is lowercased and joined back into a space-\n",
    "    separated cleaned string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text_list : list of str\n",
    "        A list containing text documents (one per row of your dataframe).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of str\n",
    "        A list of cleaned text strings corresponding to each input document, with\n",
    "        punctuation and numbers removed.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    total_docs = len(text_list)\n",
    "    cleaned = []\n",
    "\n",
    "    for doc in tqdm(\n",
    "        nlp.pipe(text_list, batch_size=500, n_process=-1),\n",
    "        total=total_docs,\n",
    "        desc=\"Cleaning text (punct+numbers)\",\n",
    "        colour=\"green\",\n",
    "        ncols=100\n",
    "    ):\n",
    "        tokens = [token.text.lower() for token in doc if token.is_alpha]\n",
    "        cleaned.append(\" \".join(tokens))\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "# Applying on column\n",
    "df_copy[\"preprocessed_text\"] = spacy_remove_punct_numbers_pipe(\n",
    "    df_copy[\"preprocessed_text\"].tolist()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de706514",
   "metadata": {},
   "source": [
    "### Lemmetization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d81b5a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lemmatizing with spaCy: 100%|\u001b[32m██████████████████████████████\u001b[0m| 159571/159571 [06:18<00:00, 421.78it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def spacy_lemmatize_pipe(text_list):\n",
    "    \"\"\"\n",
    "    Lemmatize a list of text documents using spaCy's optimized processing pipeline.\n",
    "\n",
    "    This function applies lemmatization to each text in `text_list` using spaCy's\n",
    "    `nlp.pipe()`, which supports efficient batched and parallel processing.\n",
    "    Every token in each document is replaced with its lemma form, and the lemmas\n",
    "    are joined back into a space-separated string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text_list : list of str\n",
    "        The input list of text documents to lemmatize.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of str\n",
    "        A list of lemmatized text strings, one for each input document.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    total_docs = len(text_list)\n",
    "    lemmatized = []\n",
    "\n",
    "    for doc in tqdm(\n",
    "        nlp.pipe(text_list, batch_size=500, n_process=-1),\n",
    "        total=total_docs,\n",
    "        desc=\"Lemmatizing with spaCy\",\n",
    "        colour=\"green\",\n",
    "        ncols=100\n",
    "    ):\n",
    "        lemmas = [token.lemma_ for token in doc]\n",
    "        lemmatized.append(\" \".join(lemmas))\n",
    "\n",
    "    return lemmatized\n",
    "\n",
    "\n",
    "# Apply on column\n",
    "df_copy[\"preprocessed_text\"] = spacy_lemmatize_pipe(\n",
    "    df_copy[\"preprocessed_text\"].tolist()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593f1291",
   "metadata": {},
   "source": [
    "### Converting Raw text into Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff15290d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing with spaCy: 100%|\u001b[32m███████████████████████████████\u001b[0m| 159571/159571 [06:02<00:00, 440.67it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def spacy_tokenize_pipe(text_list):\n",
    "    \"\"\"\n",
    "    Tokenize a list of text documents using spaCy's high-performance pipeline.\n",
    "\n",
    "    This function efficiently tokenizes each document in `text_list` by leveraging\n",
    "    spaCy's `nlp.pipe()` for parallel, batched processing. It returns each document\n",
    "    as a list of raw token strings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text_list : list of str\n",
    "        List of input text documents (e.g., each row of a DataFrame column).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of list of str\n",
    "        A list where each element corresponds to one document and contains the\n",
    "        tokenized word strings.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    total_docs = len(text_list)\n",
    "    tokenized = []\n",
    "\n",
    "    for doc in tqdm(\n",
    "        nlp.pipe(text_list, batch_size=500, n_process=-1),\n",
    "        total=total_docs,\n",
    "        desc=\"Tokenizing with spaCy\",\n",
    "        colour=\"green\",\n",
    "        ncols=100\n",
    "    ):\n",
    "        tokenized.append([token.text for token in doc])\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# apply on the whole column\n",
    "df_copy[\"preprocessed_text\"] = spacy_tokenize_pipe(df_copy[\"preprocessed_text\"].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd40550f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[explanation, why, the, edit, make, under, my,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[he, match, this, background, colour, I, be, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[hey, man, I, be, really, not, try, to, edit, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[more, I, can, not, make, any, real, suggestio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[you, sir, be, my, hero, any, chance, you, rem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[and, for, the, second, time, of, ask, when, y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[you, should, be, ashamed, of, yourself, that,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[spitzer, umm, there, be, no, actual, article,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[and, it, look, like, it, be, actually, you, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[and, I, really, do, not, think, you, understa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  toxic  \\\n",
       "0       Explanation\\nWhy the edits made under my usern...      0   \n",
       "1       D'aww! He matches this background colour I'm s...      0   \n",
       "2       Hey man, I'm really not trying to edit war. It...      0   \n",
       "3       \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4       You, sir, are my hero. Any chance you remember...      0   \n",
       "...                                                   ...    ...   \n",
       "159566  \":::::And for the second time of asking, when ...      0   \n",
       "159567  You should be ashamed of yourself \\n\\nThat is ...      0   \n",
       "159568  Spitzer \\n\\nUmm, theres no actual article for ...      0   \n",
       "159569  And it looks like it was actually you who put ...      0   \n",
       "159570  \"\\nAnd ... I really don't think you understand...      0   \n",
       "\n",
       "        severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0                  0        0       0       0              0   \n",
       "1                  0        0       0       0              0   \n",
       "2                  0        0       0       0              0   \n",
       "3                  0        0       0       0              0   \n",
       "4                  0        0       0       0              0   \n",
       "...              ...      ...     ...     ...            ...   \n",
       "159566             0        0       0       0              0   \n",
       "159567             0        0       0       0              0   \n",
       "159568             0        0       0       0              0   \n",
       "159569             0        0       0       0              0   \n",
       "159570             0        0       0       0              0   \n",
       "\n",
       "                                        preprocessed_text  \n",
       "0       [explanation, why, the, edit, make, under, my,...  \n",
       "1       [he, match, this, background, colour, I, be, s...  \n",
       "2       [hey, man, I, be, really, not, try, to, edit, ...  \n",
       "3       [more, I, can, not, make, any, real, suggestio...  \n",
       "4       [you, sir, be, my, hero, any, chance, you, rem...  \n",
       "...                                                   ...  \n",
       "159566  [and, for, the, second, time, of, ask, when, y...  \n",
       "159567  [you, should, be, ashamed, of, yourself, that,...  \n",
       "159568  [spitzer, umm, there, be, no, actual, article,...  \n",
       "159569  [and, it, look, like, it, be, actually, you, w...  \n",
       "159570  [and, I, really, do, not, think, you, understa...  \n",
       "\n",
       "[159571 rows x 8 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toxic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
